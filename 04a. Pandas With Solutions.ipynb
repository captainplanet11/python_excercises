{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1080/1*_oSOImPmBFeKj8vqE4FCkQ.jpeg\" title=\"Pandas\" width=\"150\" height=\"100\"/>\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is a fast, powerful, flexible and easy to use **open source Python package for data management**, built on top of the Python programming language.\n",
    "\n",
    "The word <i>pandas</i> is a creative abbreviation for Python Data Analysis Library.  The name derives from the term \"panel data\", used in econometrics to describe observations related to the same individuals over different periods of time. It was developed by Wes McKinney in 2008.  \n",
    "\n",
    "Pandas is one of the most important tools currently being used in data science applications, particularly *data wrangling*. It contains useful functions for all aspects of  **data manipulation** and is widely used for **reading data, modifying data (data wrangling and cleansing), data analysis and writing data... basically everything with data** as it provides high-performance and easy-to-use pre-written functions. \n",
    "\n",
    "Pandas is built on top of the 'NumPy' package, meaning a lot of the structure of NumPy is used or replicated in Pandas. Data in pandas is often used to feed plotting functions from `Matplotlib`, and machine learning algorithms in `Scikit-learn`.\n",
    "\n",
    "There are two types of data structures that are created in Pandas: <i>Series</i> and <i>DataFrames</i> (We will see their difference in the paragraphs below).  By creating these data structures with rows and columns, the data becomes much easier to work with.\n",
    "\n",
    "This notebook will cover the following topics:\n",
    "\n",
    "1.\tGetting Started\n",
    "    a.\tViewing Pandas Version thatâ€™s been installed in AW\n",
    "    b.\tImporting Pandas\n",
    "2.\tPandas Series \n",
    "    a.\tCreating a series\n",
    "    b.\tIdentifying Missing values within a Series\n",
    "    c.\tAdding Series together\n",
    "3.\tPandas Dataframes\n",
    "    a.\tCreating a dataframe\n",
    "    b.\tSummarizing dataframe info\n",
    "4.\tRow & Column Selection (filtering) from a dataframe\n",
    "    a.\tIndices\n",
    "    b.\tIndexing operator\n",
    "    c.\tloc & iloc\n",
    "    d.\tRow selection using a boolean mask\n",
    "    e.\tMulti-indexed dataframes (with loc and iloc)\n",
    "5.\tModifying Dataframes by Adding New Columns or Removing Columns/Rows\n",
    "6.\tFunction application and mapping\n",
    "7.\tDataFrame sorting and ranking\n",
    "8.\tData Aggregation (GroupBy operations)\n",
    "\n",
    "\n",
    "<b>Note:</b> This section is intended to be more \"hands-on\" so that the user can become more familiar with these data structures in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pandas is all about working with Data, we will be illustrating a lot of the Pandas concepts using a dataset. An entire modeling methodology is explored, starting from the basics of data exploration and treatment and ending by exploring different techniques for predictive analytics (logistic regression, decision trees, gradient boosting, etc.) The dataset we will use is a credit risk dataset containing credit card default information of clients in Taiwan. \n",
    "\n",
    "What follows is a brief description of the 25 variables in the dataset:\n",
    "<b>ID</b>: ID of each client\n",
    "<b>LIMIT_BAL</b>: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "<b>SEX</b>: Gender (1 = male; 2 = female).\n",
    "<b>EDUCATION</b>: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "<b>MARRIAGE</b>: Marital status (1 = married; 2 = single; 3 = others).\n",
    "<b>AGE</b>: Age (year).\n",
    "\n",
    "History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows:\n",
    "\n",
    "<b>PAY_0</b>:  the repayment status in September, 2005;\n",
    "<b>PAY_2</b>: the repayment status in August, 2005; . . .;\n",
    "<b>PAY_3</b>: . . .\n",
    "<b>PAY_4</b>: . . .\n",
    "<b>PAY_5</b>: . . .>\n",
    "<b>PAY_6</b>: the repayment status in April, 2005. \n",
    "The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
    "\n",
    "Amount of bill statement (NT dollar).\n",
    "\n",
    "<b>BILL_AMT1</b>: amount of bill statement in September, 2005;\n",
    "<b>BILL_AMT2</b>: amount of bill statement in August, 2005; . . .;\n",
    "<b>BILL_AMT3</b>: . . .;\n",
    "<b>BILL_AMT4</b>: . . .;\n",
    "<b>BILL_AMT5</b>: . . .;\n",
    "<b>BILL_AMT6</b>: amount of bill statement in April, 2005.\n",
    "\n",
    "Amount of previous payment (NT dollar).\n",
    "\n",
    "<b>PAY_AMT1</b>: amount paid in September, 2005;\n",
    "<b>PAY_AMT2</b>: amount paid in August, 2005; . . .;\n",
    "<b>PAY_AMT3</b>: . . .;\n",
    "<b>PAY_AMT4</b>: . . .;\n",
    "<b>PAY_AMT5</b>: . . .;\n",
    "<b>PAY_AMT6</b>: amount paid in April, 2005;\n",
    "<b>default.payment.next.month</b>: payment default (1 = yes; 2 = no)\n",
    "\n",
    "Source: UCI ML Repository: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by confirming that the `Pandas` library has already been installed in this Analytics Workbench Tenant. Don't worry about understanding the code in the next paragraph- it's a popular snippet you'll want to copy and re-use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import platform, sys, subprocess, re\n",
    "print(\"Platform OS: {} version {}\".format(platform.system(),platform.release()))\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "\n",
    "cmd = [sys.executable, '-m', 'pip', 'freeze']\n",
    "pkgs = []\n",
    "my_keys = ['Module', 'Version']\n",
    "for line in subprocess.check_output(cmd).decode(\"utf-8\").splitlines():\n",
    "    pkgs.append( re.split('==', str(line), len(my_keys)-1) )\n",
    "\n",
    "# print in a nice Zeppelin table\n",
    "print(\"These {} modules are available in Python:\".format(len(pkgs)))\n",
    "print(\"%table\\n{}\\t{}\".format(*my_keys))\n",
    "for i in pkgs:\n",
    "        print(\"{}\\t{}\".format(*i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the `pandas` library into our notebook.  Pandas is almost always referenced as **pd** so it's best to stick with this convention (making it easier to run code somebody else wrote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series in `pandas` is a one dimensional array or vector of data. Series are one of the 2 fundamental data structures in `Pandas` and can store values of any type (integers, strings, etc.) with a unique index (label) per value. Series can be thought of as the columns in a table (or dataframe which we'll learn more about in a moment).  More specifically, pandas series are `ndarrays` (see the <i>NumPy</i> class). Their general form follows the syntax: \n",
    "\n",
    "````\n",
    "import pandas as pd\n",
    "pd.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)\n",
    "````\n",
    "In the real world, a Pandas Series will be created by loading the datasets from existing storage (CSV file). Pandas Series can also be created from the lists, dictionaries etc. Below, we will provide example code to do the following:\n",
    "\n",
    "* Generatea a `pandas` Series object from a range object, using a manually entered list as the index.\n",
    "* Transform a dictionary to a series (and vice-versa)\n",
    "* Operations on series:\n",
    "    * Returning a flag indicating which values are null\n",
    "    * Adding 2 series together\n",
    "\n",
    "A key concept mentioned above is that of an *index*:\n",
    "\"Pandas is a best friend to a Data Scientist, and index is the invisible soul behind pandas\"- Some wise person on the internet\n",
    "\n",
    "The concept of an index is critical for many of the pandas methods such as loc, iloc, filtering, stack/unstack, concat, merge, pivot, etc.  You can think of an index like a label that gets applied to your rows and column. For rows, the index is the row label uniquely identifying each row/  For columns, the index is the column name/header.\n",
    "\n",
    "We'll then quickly move on to dataframes, because that's where the real fun is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#Series is a one dimensional array-like object. It also contains the index along with each value- by default, the indices are the same as the position number (starting at 0)\n",
    "obj = pd.Series(range(5))\n",
    "obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# We can also provide our own index name into a Series.\n",
    "obj = pd.Series(range(5), index=['a','b','c','d','e'])\n",
    "obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Considering that a Series internal structure is given by an `ndarray`, what is the output of this?\n",
    "````\n",
    "import numpy as np\n",
    "obj = pd.Series(np.arange(5),index=['a','b','c','d','e'])\n",
    "obj[['c','e']]\n",
    "````\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obj = pd.Series(np.arange(5),index=['a','b','c','d','e'])\n",
    "obj[['c','e']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series can be transformed to a dictionary and vice-versa. The general expression is as follows: \n",
    "\n",
    "````\n",
    "# from Series to dictionary\n",
    "Series.to_dict(into=<class 'dict'>)\n",
    "\n",
    "# from dictionary to Series\n",
    "pd.Series(dictionary)\n",
    "````\n",
    "\n",
    "For example, we can convert the following dictionary to a Series.\n",
    "````\n",
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "\n",
    "obj = pd.Series(sdata)\n",
    "\n",
    "obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following Series.\n",
    "````\n",
    "sdata = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "obj = pd.Series(sdata)\n",
    "````\n",
    "In the NumPy section, we saw that a missing value (`np.nan`) can be identified with the following: \n",
    "````\n",
    "np.isnan(x)\n",
    "````\n",
    "In `pandas` we could do something similar: \n",
    "````\n",
    "pd.isnull(x)\n",
    "````\n",
    "We can write a script to identify all the missing values in the dictionary above, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sdata = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "\n",
    "obj = pd.Series(sdata)\n",
    "\n",
    "pd.isnull(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do different operations as well, using Series object. For example, let's considering the following two Series:\n",
    "````\n",
    "sdata1 = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "obj1 = pd.Series(sdata1)\n",
    "sdata2 = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "obj2 = pd.Series(sdata2)\n",
    "````\n",
    "What will be the output of obj1 + obj2?\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sdata1 = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "obj1 = pd.Series(sdata1)\n",
    "sdata2 = {'Ohio': np.NaN, 'Texas': 71000, 'Oregon': np.NaN, 'Utah': 5000}\n",
    "obj2 = pd.Series(sdata2)\n",
    "obj1 + obj2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is a 2-dimensional table-like structure, comprised of rows and columns. To each column is associated a name (like a column header) and to each row is associated an index (row label). It resembles a SQL table and can be seen as a dictionary of Series objects (each column can be viewed as a `pandas` Series). It is the most used pandas object and can be created in many ways: from existing dictionaries, 2D-ndarrays, Series or other DataFrames.  Most commonly, DataFrames are created from a (CSV) file. Its general structure is: \n",
    "\n",
    "````\n",
    "pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)\n",
    "````\n",
    "\n",
    "The dataframe is the primary pandas data structure and from now on we will mostly work with DataFrames for data wrangling, data cleansing, data enrichment and model development. \n",
    "\n",
    "The key difference between a pandas DataFrame object and a pandas Series object is that a Series is the data structure for a *single* column of a DataFrame (the data in a DataFrame is actually stored in memory as a collection of Series).\n",
    "\n",
    "In this section we will provide examples to do the following: \n",
    "\n",
    "* Create a DataFrame from an existing dictionary\n",
    "* Create a Dataframe from external data (a CSV file)\n",
    "* Summarize the information in a dataframe\n",
    "\n",
    "We'll also introduce a couple useful methods to manipulate (in a pretty blunt way) the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "my_dict = {'id':[1,2,3,4,5],\n",
    "           'name':['Calvin','Ron','Daisy','Jasmine','Juan'],\n",
    "           'income':[20000,40000,20000,10000,50000]}\n",
    "\n",
    "df = pd.DataFrame(my_dict)#,columns=['id','income','name'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above dataframe, the values in the column index (top row) are the same as the column names and the values in the row index (first column) are the row position number (starting at 0).\n",
    "\n",
    "We can also specify the values that are used for the row index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "my_dict = {'id':[1,2,3,4,5],\n",
    "           'name':['Calvin','Ron','Daisy','Jasmine','Juan'],\n",
    "           'income':[20000,40000,20000,10000,50000]}\n",
    "\n",
    "df = pd.DataFrame(my_dict,columns=['id','income','name'],index=['row0','row1','row2','row3','row4',])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a lot of methods to read external data and transform to dataframe\n",
    "\n",
    "```Python\n",
    "pd.read_csv(loc)\n",
    "pd.read_json(loc)\n",
    "pd.read_html(loc)\n",
    "pd.clip_board(loc)\n",
    "pd.read_excel(loc)\n",
    "pd.read_sas(loc)\n",
    "pd.read_pickle(loc)\n",
    "pd.read_sql(loc)\n",
    "pd.read_spss(loc)\n",
    "\n",
    "#And others\n",
    "```\n",
    "Here (and most of the time in the real world) We are going to use the ``pd.read_csv()`` to read an external data, which has the following syntax:\n",
    "\n",
    "```Python\n",
    "pd.read_csv(filepath_or_buffer, sep,  decimal, names, dtype,  header, index_col, prefix, na_values )\n",
    "```\n",
    "\n",
    "It can read \".txt\" and \".csv\" files and has many many more options than this, but here are the most useful:\n",
    "\n",
    "| --- | --- | --- |\n",
    "| **Parameter** | **Default Value** |**Defintion** |\n",
    "| filepath_or_buffer | required value | The full file path in string format |\n",
    "| sep | ',' | a string that will be used to detect the column delimiter |\n",
    "| decimal | '.' |Character to recognize as decimal point. |\n",
    "| names | None |a list containing the unique column names (if it's not in the data) |\n",
    "| dtype | None |a optional dictionary to explicity tell Python the data format for each column |\n",
    "| header | 0 |Row number to use as columns names. Put equal ``None`` if you don't want to use this parameter |\n",
    "| index_col | None | optional column name/number (or list of names/number) to be used as index. |\n",
    "| prefix | None | optional string if you want to add a prefix to every column names |\n",
    "| na_values | None |optional list containing numbers or strings to be identified as ``null`` values |\n",
    "\n",
    "\n",
    "Inside AW3.0 Python can read data directly from S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Importing required libraries\")\n",
    "import pandas as pd\n",
    "import os, boto3, subprocess, re, sys, gc\n",
    "from botocore.client import Config\n",
    "\n",
    "print(\"All libraries successfully loaded!\")\n",
    "\n",
    "kms_key = os.environ['AW_S3_ENCRYPTION_KEY']\n",
    "\n",
    "bucket_name = os.environ['AW_S3_STORAGE_BUCKET']\n",
    "storage_key = os.environ['AW_S3_STORAGE_KEY'] + '/awdata/rawfiles/'\n",
    "full_s3_location = 's3://' + bucket_name + '/' + storage_key \n",
    "print(\"full_s3_location: '{}'\".format(full_s3_location))\n",
    "\n",
    "df_twn= pd.read_csv(full_s3_location + \"UCI_Credit_Card.csv\",nrows=100)\n",
    "z.show(df_twn.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's shown below are some common methods to get quick information about the data contained in a particular dataframe (`describe()`, `info()`, `shape`, `index`, `columns.values`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.shape\n",
    "print(df_twn.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.index\n",
    "# print(df_twn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "## find all integers\n",
    "df_twn.select_dtypes('int64')\n",
    "\n",
    "## find all numerics\n",
    "# df_twn.select_dtypes(['int64','float64'])\n",
    "## fina all strings\n",
    "# df_twn.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# transform the data format\n",
    "# Tranform all data to float64\n",
    "df_twn.astype('str',inplace=False)\n",
    "# or:\n",
    "# 'float64'\n",
    "# 'int64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a single column from a DataFrame (a Series), one can simply access  it using its name. For example, given a DataFrame \"df\" and a  column \"col\", the following code outputs a pandas Series (single column) originating from a DataFrame called df:  \n",
    "\n",
    "````\n",
    "df['col']\n",
    "````\n",
    "\n",
    "Another, equivalent, way to do this is the following.  \n",
    "````\n",
    "df.col\n",
    "````\n",
    "However, the above code requires \"col\" to be a valid python name- so df.column works, but df.column name would cause an error and you would need to use df['column name']\n",
    "\n",
    "To return a *dataframe* instead of a series, you can pass in 'col1' as a 1-element list:\n",
    "````\n",
    "df[['col1']]\n",
    "````\n",
    "\n",
    "Of course you can pass in multi-element lists e.g. ['col1','col2','col3'] as well- which will return a dataframe with *multiple columns* which is a subset of the original\n",
    "````\n",
    "df[['col1','col2','col3']]\n",
    "````\n",
    "\n",
    "To select a subset of columns *and* rows by their index values (labels), we use the method `loc[]`. To select a subset of columns *and* rows by their numeric locations (starting at 0) we use the method `iloc[]`: \n",
    "\n",
    "````\n",
    "df.loc[row_labels, col_labels]\n",
    "df.iloc[row_positions, col_positions]\n",
    "````\n",
    "Finally, we will show how to select rows using a boolean mask: \n",
    "````\n",
    "df[boolean_mask]\n",
    "````\n",
    "\n",
    "We will see these two methods more in-depth  in the following sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` method is used to fetch the specified rows and columns by labels. First we specify the row labels we want to keep, then the column labels we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "my_dict = {'id':[1,2,3,4,5],\n",
    "           'name':['Calvin','Ron','Daisy','Jasmine','Juan'],\n",
    "           'income':[20000,40000,20000,10000,50000]}\n",
    "\n",
    "df = pd.DataFrame(my_dict,columns=['id','income','name'],index=['row0','row1','row2','row3','row4',])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output:\n",
    "\n",
    "````\n",
    "df.loc[['row0'],['id','income']]\n",
    "````\n",
    "\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.loc[['row0'],['id','income']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iloc method is used to fetch the specified rows and columns by index. First we specify the row numeric positionsnumeric positions we want to keep, then the column labels we want to keep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output:\n",
    "\n",
    "````\n",
    "df.iloc[[0,1],[1]]\n",
    "````\n",
    "\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.iloc[[0,1],[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass a slice to both loc and iloc. \n",
    "\n",
    "* For `loc`, the slice is left-inclusive and right- __inclusive__\n",
    "* For `iloc`, the slice is left-inclusive and right- __exclusive__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# df.loc['row2':'row3','income':'name']\n",
    "df.iloc[1:2,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *extreme* case where you don't specify any index values or column names, loc and iloc give the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "# create a 4x4 matrix of numbers- note that the labels and the numeric indices are the same\n",
    "df = pd.DataFrame(np.arange(16).reshape(4,4))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.loc[[0,3],[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.iloc[[0,3],[0,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the following DataFrame, how can we access the values of the <b>income</b> column? \n",
    "\n",
    "\n",
    "|   | id | name    |income|\n",
    "| - |:--:| --------:|:---:|\n",
    "| 0 | 1  | Calvin   |20000|\n",
    "| 1 | 2  | Ron      |40000|\n",
    "| 2 | 3  | Daisy    |20000|\n",
    "| 3 | 4  | Jasmine  |20000|\n",
    "| 4 | 5  | Juan     |20000|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':[1,2,3,4,5],'name':['Calvin','Ron','Daisy','Jasmine','Juan'],'income':[20000,40000,20000,10000,50000]})\n",
    "print(df)\n",
    "print(\"method 1:\", df['income'])\n",
    "print(\"method 2:\",df.income)\n",
    "print(\"method 3:\",df.loc[:,'income'])\n",
    "print(\"method 4:\",df.iloc[:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our Taiwan dataset described above (df_twn), use pandas to answer the following questions about the data:\n",
    "\n",
    "1. How many columns and rows does the data have?\n",
    "2. How much memory (RAM) do you need to read this data?\n",
    "3. Are there any columns with incorrect default data types? If so, correct them.\n",
    "4. How many variables do we have by type (numerical, string) (answer this *without* using ``.info()`` or ``.dtypes``)\n",
    "5. Create two describes, one for numeric variables and one for string variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows:\", df_twn.shape[0])\n",
    "print(\"Number of columns:\", df_twn.shape[1])\n",
    "# The first position of the shape method is the number of rows and the second is the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_twn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df_twn.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "should_be_str = ['SEX','EDUCATION', 'MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','default.payment.next.month']\n",
    "df_twn[should_be_str] = df_twn[should_be_str].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print('Number of numerical variables:', len(df_twn.select_dtypes(['int64','float64']).columns.values))\n",
    "print('Number of string variables:', len(df_twn.select_dtypes(['object']).columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(df_twn.select_dtypes(['float64','int64']).describe())\n",
    "\n",
    "\n",
    "# Unfortunately, if you use the z.show() it will not show the row index. Because of that, we need to use the .reset_index() method which creates a new default index and (as long as you keep the default setting of 'drop=False')  shifts the old index to the first data column (and shifts all existing columns).\n",
    "\n",
    "z.show(df_twn.select_dtypes(['float64','int64']).describe().reset_index())\n",
    "\n",
    "# print(df_twn.select_dtypes(['float64','int64']).describe().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df_twn.select_dtypes(['object']).describe().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select (filter) rows from a dataframe using a Boolean mask.\n",
    "\n",
    "What will be the output of the following?\n",
    "````\n",
    "df[df.income > 10000]\n",
    "````\n",
    "On this dataframe:\n",
    "\n",
    "|   | id | name    |income|\n",
    "| - |:--:| --------:|:---:|\n",
    "| 0 | 1  | Calvin   |20000|\n",
    "| 1 | 2  | Ron      |40000|\n",
    "| 2 | 3  | Daisy    |20000|\n",
    "| 3 | 4  | Jasmine  |20000|\n",
    "| 4 | 5  | Juan     |20000|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':[1,2,3,4,5],'name':['Calvin','Ron','Daisy','Jasmine','Juan'],'income':[20000,40000,20000,10000,50000]})\n",
    "print(\"df: \",df)\n",
    "print(\"\\nboolean mask\\n\", [df.income > 10000])\n",
    "print(\"\\ndf filtered on the boolean mask\\n\",  df[df.income > 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the (raw) Taiwan data (df_twn) and using one line of code and the `.loc` method, show only the numeric columns for the first 4 records records which have **LIMIT_BAL**  > 300000 and **AGE** > 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# print(\"first boolean mask: \",df_twn.LIMIT_BAL>300000)\n",
    "# print(\"second boolean mask: \",df_twn.AGE>30)\n",
    "# print(\"final boolean mask: \",(df_twn.LIMIT_BAL>300000) & (df_twn.AGE>30))\n",
    "# print (\"use loc to select rows based on final boolean mask: \", df_twn.loc[(df_twn.LIMIT_BAL>300000) & (df_twn.AGE>30),:])\n",
    "#Finally, use select_dtypes to select just the numeric columns and include head in teh z.show\n",
    "z.show(df_twn.loc[(df_twn.LIMIT_BAL>300000) & (df_twn.AGE>30),:].select_dtypes([\"int64\", 'float64']).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a boolean mask to indicate which rows to keep (True) and drop (False)\n",
    "2. Use loc with the boolean vector to select the rows to keep\n",
    "3. Select the numeric columns from the above result\n",
    "4. Show the first 4 rows from the above result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,5),\n",
    "                  index=[['a','a','a','b','b','b','c','c','d','d'],\n",
    "                         [ 1 , 2 , 3 , 1 , 2 , 3 , 1 , 2 , 1 , 2]])\n",
    "\n",
    "print(\"Index: \", df.index)\n",
    "print(\"df\",df)                         \n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "## Accessing all the values within the index value of 'a'\n",
    "\n",
    "# print(\"df.loc['a',:] \", df.loc['a',:])\n",
    "\n",
    "## Accessing the 2nd & 3rd row \n",
    "\n",
    "print(\"\\ndf.iloc[1:3,:] \",df.iloc[1:3,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output of the following command? \n",
    "````\n",
    "df.loc['a',].loc[1,:]\n",
    "````\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.loc['a',].loc[1,:]\n",
    "# df.swaplevel(0,1).loc[1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess how many rows the output will have given the expression below? \n",
    "````\n",
    "df.sum(level = 0)\n",
    "````\n",
    "\n",
    "#### <font color=red>PLEASE *DON'T* CODE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(df.sum(level=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please continue on to the 04b. Pandas (With Solutions) Notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
