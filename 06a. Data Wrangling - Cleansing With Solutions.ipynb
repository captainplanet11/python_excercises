{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world applications it is very common to receive data with lots of unpopulated values. Most of the default built-in functions in `pandas` ignores the missing values and compute statistics in various forms. In a pandas DataFrame, missing numerical values are represented by `NaN` (`np.nan`). These are sentinel values and can be detected with a method that we have already seen: \n",
    "\n",
    "````\n",
    "pd.isnull()\n",
    "````\n",
    "\n",
    "For example, consider the following `pandas` Series: \n",
    "````\n",
    "Series = pd.Series(['apple','banana','watermelon',np.nan,'grape'])\n",
    "````\n",
    "We can test whether a value in the Series is missing using the expression we saw before: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Now that we have a solid understanding of the fundamental structures of the Python language and its Data Science packages, we can explore applications on more realistic scenarios. Most of the time, the modeling work of a Data Scientist consists of preparing the data (importing, cleaning, combining and wrangling various datasets). This takes roughly \\\\(80\\%\\\\) of the modeling work time. We will explore in this section the tools to handle missing data, manipulate strings, transform the data and some other methods.\n",
    "\n",
    "<b>Note:</b> This section is intended to be more \"hands-on\" so that the user can become more familiar with these techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Series = pd.Series(['apple','banana','watermelon',np.nan,'grape'])\n",
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(Series)\n",
    "print(' ')\n",
    "Series.isnull()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert `None` into our previous `pandas` Series in the first position. We can then apply the method `isnull()` to check and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "Series[0] = None\n",
    "Series.isnull()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example exercises\n",
    "\n",
    "Let's put in practice some of the methods for data cleaning in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output of the following code:\n",
    "````\n",
    "%python\n",
    "import pandas as pd\n",
    "Series = pd.Series([1,2,3,np.nan,None,3.7,None,np.nan,100.3])\n",
    "Series.isnull()\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Series = pd.Series([1,2,3,np.nan,None,\n",
    "                    3.7,None,np.nan,100.3])\n",
    "Series.isnull()\n",
    "\n",
    "# The \".isnull\" method can be used for both Series and Dataframes from Pandas library.\n",
    "# it always return True if the element is null and False if it's not.\n",
    "# Note - For python, every \"True\" value is equal 1, so if you apply the \"sum\" method in this script it will return the number of null rows.\n",
    "\n",
    "# Numpy is a great library for both number generator and mathematical programming. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored here and in previous sections, methods to analyze and to <b>detect</b> missing values. However, we have not yet explored any method that filters out the missing values. Pandas provides a useful solution: \n",
    "````\n",
    "## To return only the non-null data as well as the index of a Series, one can use: \n",
    "Series.dropna()\n",
    "\n",
    "## This is equivalent to: \n",
    "Series[Series.notnull()]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output of the following code:\n",
    "\n",
    "````\n",
    "%python\n",
    "\n",
    "Series = pd.Series([1,2,3,np.nan,None,303.5])\n",
    "Series.dropna()\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "Series = pd.Series([1,2,3,np.nan,None,303.5])\n",
    "Series.dropna()\n",
    "\n",
    "# The \"dropna()\" method canbe used for both Series and DataFrames and it will drop all the na elements from the Series (by default it will not change the original object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output of the following code:\n",
    "````\n",
    "Series = pd.Series([1,2,3,np.nan,None,303.5])\n",
    "Series[Series > 100]\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Series = pd.Series([1,2,3,np.nan,None,303.5])\n",
    "Series[Series < 100]\n",
    "\n",
    "# Here, both \"np.nan\" and \"None\" are considered as NULL values. If we use a logical mask to filter our Series or Dataframe at some value ranges it will always delete the null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output of the following code:\n",
    "\n",
    "````\n",
    "%python\n",
    "\n",
    "Series[Series.isnull()]\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "\n",
    "Series[Series.isnull()]\n",
    "# By using the isnull method, we are creating a boolean mask. That way we can find all the NULL values within our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output of the following code:\n",
    "\n",
    "````\n",
    "%python\n",
    "\n",
    "Series[Series.notnull()]\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "\n",
    "Series[Series.notnull()]\n",
    "#Series[~Series.isnull()]\n",
    "\n",
    "# The method \".notnull\" do the opposite as \".isnull\" method. It will return only the rows that aren't null.\n",
    "# It can be used in both Series and Dataframe pandas objects\n",
    "# Other way to have the same result is by doing \"~Series.isnull()\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame objects we can specify whether we want to drop rows or columns where all the values are \"NaN\" or only the ones containting NaN. By default, `dropna()` drops the rows containting at least one missing value.\n",
    "\n",
    "We can specify the method within the `dropna()` function, such as `thresh=3` (rows that contain a certain amount of information - in this case 3 non-null values) and `how=all` (rows where all the values are \"NaN\"). Let's see some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "### Let's consider the following dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame([[3.,8.3,np.nan,8.7],[2.,np.nan,4.,4.5],[np.nan,8.9,4.,7.8],[np.nan,np.nan,10.2,10.2],[1.2,3.7,100.9,7.1]])\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output if we apply `dropna()` to the previous dataframe? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "df.dropna()\n",
    "\n",
    "# the \".dropna()\" method can be a little agressive in the default configuration because it will return only the lines without null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we apply the following? \n",
    "\n",
    "````\n",
    "df.dropna(how='all')\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "df.dropna(how='all')\n",
    "\n",
    "# The option \"how = 'all'\" make the \".dropna()\" delete all the rows that have nulls in all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the 2nd <b>and</b> 3rd columns in the 3rd line were missing?\n",
    "````\n",
    "%python\n",
    "df.dropna(how='all')\n",
    "````\n",
    "#### `PLEASE DON'T CODE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "df_2 = pd.DataFrame([[3.,8.3,np.nan,8.7],[2.,np.nan,4.,4.5],[np.nan,8.9,4.,7.8],[np.nan,np.nan,np.nan,np.nan],[1.2,3.7,100.9,7.1]])\n",
    "df_2.dropna(how='all')\n",
    "\n",
    "# Since the third line have all columns equal null it will be deleted from the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead, we want to drop columns with missing values we could do the following:\n",
    "````\n",
    "%python\n",
    "df.dropna(axis=1)\n",
    "````\n",
    "What's the output of the previous code? \n",
    "\n",
    "#### `PLEASE DON'T CODE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "df.dropna(axis=1)\n",
    "\n",
    "# By default, the \".dropna()\" method will look for rows with nulls\n",
    "# But if we change the axis to 1 it will look for columns, that way it will return only the columns without null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output of the following script? \n",
    "````\n",
    "%python\n",
    "df.dropna(axis=1,how='all')\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER:`\n",
    "df.dropna(axis=1,how='all')\n",
    "\n",
    "# It will drop the columns that have null values in all rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,5))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.iloc[:3,1:3] = np.nan\n",
    "df.iloc[7:10,0:3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which rows will remain after we apply `dropna()` to the previous dataframe? \n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ANSWER:`\n",
    "\n",
    "Rows \\\\(3, 4, 5\\\\) and \\\\(6\\\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned previously about the method `thresh` to control the minimum number of non-missing values to be kept per row. What's the output? \n",
    "\n",
    "````\n",
    "%python\n",
    "\n",
    "df.dropna(thresh=4)\n",
    "````\n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh= int(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ANSWER:`\n",
    "\n",
    "Rows \\\\(3, 4, 5\\\\) and \\\\(6\\\\).\n",
    "\n",
    "The ``thresh`` is a optional parameter and it will specify how many non-NUll values the line has to be to appear. So if we have ``thresh = 3`` it will return all row that have at least three valid values (non-NULL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the previous section how to remove missing values in various forms. However, very often, a lot of valuable information is lost if we simply remove rows or columns with a relatively small amount of missing. It is better, instead, to in fill the \"gaps\" with a well-defined criterion. Pandas provides a built-in method: `fillna()`. It fills the gaps with a a given value. For example, if we use the following, we fill in the gaps with 0.\n",
    "````\n",
    "df.fillna(0)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "df.fillna(0)\n",
    "\n",
    "# The \".fillna()\" method will not change the original dataframe, it will output only a print with the transformed dataframe.\n",
    "# To change you Dataframe permanently you can use the option \"inplace = True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify the specific condition we want to use to fill different columns. We can associate any dictionary to any column, such as: \n",
    "````\n",
    "dictionary = {'Column1':Value1,...,'ColumnN':ValueN} \n",
    "df.fillna(dictionary)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script to fill the missing values of column \\\\(0\\\\) with \\\\(1\\\\) and the remaining columns with \\\\(0\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.fillna({   0 : 1\n",
    "            , 1 : df[1].mean()\n",
    "            , 2 : 0\n",
    "            , 3 : 0\n",
    "            , 4 : 0})\n",
    "\n",
    "df\n",
    "\n",
    "# It's filling the first column with 1s where we have a null value\n",
    "# and for all the other columns it is filling with 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "What happens if we use `fillna` with `inplace`? What's an example of equivalent code to what's shown below? \n",
    "````\n",
    "df.fillna(0,inplace=True)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ANSWER:`\n",
    "If we use `inplace`, the changes are made in the original dataframe. This is equivalent to `df = df.fillna(0)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other useful methods available in `pandas` to execute fast \"filling\" strategies: `ffill` and `bfill`. This can be set such as: \n",
    "````\n",
    "df.fillna(method='ffill',inplace=True)\n",
    "df.fillna(method='bfill',inplace=True)\n",
    "````\n",
    "To explore a bit futher the methods available to fill in missing values in a dataset, let's consider the following dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(8,4))\n",
    "df.iloc[3:7,1:3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.fillna(method='ffill',limit=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to the previous dataframe when we used the methods '`ffill`' and '`limit`'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ANSWER:`\n",
    "The missing values were replaced ('filled') with the previous non-missing value of each column ('ffill' stands for forward fill). In this case, for both 1 and 2 columns it is taking the values from the line 2 and filling the next 3 rows.\n",
    "\n",
    "if we change the limit to 4 it will fill the next four rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that `bfill` does the opposite of `ffill`, predict the output of the following code:\n",
    "````\n",
    "%python\n",
    "\n",
    "df.fillna(method='bfill',limit=3)\n",
    "````\n",
    "#### `PLEASE DON'T CODE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#### `ANSWER`\n",
    "\n",
    "df.fillna(method='bfill', axis = 1)\n",
    "\n",
    "# the \"bfill\" method is doing the exactly opposite from \"ffill\" method, i.e., it is taking the value below the last NULL value and it's replacing three NULL values above this number. (because limit = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to fill in the missing values of a pandas dataframe with any function we want. For example, one can fill in the missing values with built-in functions, such as `mean()`, `max()`, or any other mathematical expression applied to a wide variety of data values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code to fill the `NaN` values of the previous dataframe with the average values of the following series:\n",
    "````\n",
    "Series = pd.Series([1.2,np.nan,None,3.5,10.8])\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "Series = pd.Series([1.2,np.nan,None,3.5,10.8])\n",
    "df.fillna(Series.mean())\n",
    "\n",
    "# Inside the fillna() we can put a lot of different functions. Try to put other functions like var(), std(), min().\n",
    "\n",
    "# We can use customized solutions using the apply method\n",
    "# df.fillna(df.apply(lambda x: max(x) - min(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def change_null_numeric(df, function = 'mean'):\n",
    "    df_aux = df.copy()\n",
    "    cols = df_aux.select_dtypes(['float64','int64']).columns.values\n",
    "    \n",
    "    for col in cols:\n",
    "        if function == 'mean':\n",
    "        \n",
    "            df_aux[col].fillna(np.mean(df_aux[col]), inplace = True)\n",
    "            \n",
    "    return df_aux\n",
    "    \n",
    "change_null_numeric(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that we have dealt with so far in this section involved rearranging the data. Data cleaning and filtering is also absolutely critical to any work of a Data Scientist. Here, we explore another method: `drop_duplicates()`. The `duplicated()` method checks whether one or more lines of a dataframe are duplicated (repeated). It returns a boolean series as shown below. We can use `drop_duplicates()` to eliminate the identified duplicate rows. Let's start with the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A' : ['FirstStr','SecondStr','ThirdStr',\n",
    "                          'FourthStr','FifthStr','SixthStr','SeventhStr'],\n",
    "                   'B': [1,2,3,4,5,6,6],'C' : [1,2,3,4,5,6,6],'D': [6,5,4,3,2,6,6]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.duplicated()\n",
    "# the \".duplicated()\" method will return True if it find any duplicated (or more) values in a column except for the first occurrence (by default)\n",
    "\n",
    "# use the \"keep\" option to change this:\n",
    "# keep = 'last': excpt the last occurrence will False\n",
    "# keep = 'all': all duplicates will be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the output if we had applied `drop_duplicates()` only for columns 'B','C' and 'D'? \n",
    "````\n",
    "%python\n",
    "\n",
    "df.drop_duplicates(['B','C','D'])\n",
    "````\n",
    "\n",
    "#### `PLEASE DON'T CODE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(['B','C','D'], keep = 'last')\n",
    "\n",
    "# We can choose some columns by given a sequence of labels to the \"drop_duplicates\" method.\n",
    "\n",
    "# This method can also be used for Series objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> as we can see, the last row is removed, given that everything except column A is a duplicate of row 5. By default it keeps the first duplicated row. In the example case above it kept row 5. If we add the following parameter the last row is kept instead. \n",
    "````\n",
    "keep = 'last'\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, the transformation we need to perform on a DataFrame is much more complex, and it requires us to create a new function, or to map values in a more robust way. Consider the following fictitious DataFrame: \n",
    "````\n",
    "      Company  Quantity  randVals\n",
    "0     FERRARI        40         0\n",
    "1  VOLKSWAGEN        30         1\n",
    "2     PORSCHE       120         1\n",
    "3  Volkswagen        60         0\n",
    "4        ford        70         1\n",
    "5        Ford        81         1\n",
    "6     Ferrari        32         1\n",
    "7     Peugeot        53         0\n",
    "8     Renault        62         0\n",
    "````\n",
    "Consider the following mapping dictionary: \n",
    "````\n",
    "brand_to_country = {'Ferrari':'Italy','Volkswagen':'Germany','BMW':'Germany','Ford':'USA','Peugeot':'France','Renault':'France'}\n",
    "````\n",
    "We can see that the name format for the \"Company\" column is not \"standardized\", that is, some words are all in lower case, others in upper case. The following methods allows us to easily convert the strings to a single format: \n",
    "\n",
    "````\n",
    "## all lower case\n",
    "str.lower()\n",
    "\n",
    "## all capital\n",
    "str.upper()\n",
    "\n",
    "## title style (first letter capital, others lower case)\n",
    "str.title()\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the dataframe shown above from scratch and normalize the names using the string methods shown above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "df = pd.DataFrame({'Company': ['FERRARI','VOLKSWAGEN','PORSCHE','Volkswagen',\n",
    "                               'ford','Ford','Ferrari','Peugeot','Renault'],\n",
    "                   'Quantity': [40, 30, 120, 60, 70, 81, 32, 53, 62],\n",
    "                   'randVals' : [0, 1, 1, 0, 1, 1, 1, 0, 0]})\n",
    "\n",
    "print('Standardizing word format: ')\n",
    "\n",
    "df.Company = df.Company.str.upper()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# here the \"Company\" variable is selected using the follow method \"df.Company\" and this is the same as \" df['Company'] \", so it's returning a Series object.\n",
    "\n",
    "# the .str method only works for series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Company': ['FERRARI','VOLKSWAGEN','PORSCHE','Volkswagen',\n",
    "                               'ford','Ford','Ferrari','Peugeot','Renault'],\n",
    "                   'Quantity': [40, 30, 120, 60, 70, 81, 32, 53, 62],\n",
    "                   'randVals' : [0, 1, 1, 0, 1, 1, 1, 0, 0]})\n",
    "\n",
    "#df.Company.str.split(\"o\")\n",
    "#df['Company'].str.split(\"o\").str.join(\"o\")\n",
    "#df['Company'].str.len()\n",
    "#df['Company'].str.contains(\"Ferrari\")\n",
    "#df['Company'].str.cat([\" Model\"]*df['Company'].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built-in function `map` that we've learned in the Functions module, write a script to map the Company's names to their respective country of origin and create a new column to the original dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "brand_to_country = {'Ferrari':'Italy',\n",
    "                    'Volkswagen':'Germany',\n",
    "                    'Porsche':'Germany',\n",
    "                    'Ford':'USA',\n",
    "                    'Peugeot':'France',\n",
    "                    'Renault':'France'}\n",
    "                    \n",
    "#df['Company'].map(lambda x: brand_to_country[x.title()])\n",
    "\n",
    "## adding as a new column\n",
    "df['Country'] = df['Company'].map(lambda x: brand_to_country[x.title()])\n",
    "df\n",
    "\n",
    "# The map method works only for Series.\n",
    "# We can use \"applymap\" for DataFrames\n",
    "\n",
    "# We can apply some method in all columns by using \"applymap\" or \"apply\"\n",
    "#df.select_dtypes(\"object\").apply(lambda x: x.str.title())\n",
    "#df.select_dtypes(\"object\").applymap(lambda x: x.title())\n",
    "\n",
    "#Note that applymap is doing a \"element-wise\" transformation, i.e. it's going colum-by-column and row-by-row and apply the function, that's why we just need to use x.title() because every \"x\" is only a string object\n",
    "\n",
    "# If we are using the \"apply\" we need to put x.str.title because every \"x\" is a Series.\n",
    "\n",
    "# We can use both ways, in this case, to solve the same problem. But some times is better to use one instead of the other,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the previous dataframe:\n",
    "\n",
    "````\n",
    "      Company  Quantity  randVals  Country\n",
    "0     Ferrari        40         0    Italy\n",
    "1  Volkswagen        30         1  Germany\n",
    "2     Porsche       120         1  Germany\n",
    "3  Volkswagen        60         0  Germany\n",
    "4        Ford        70         1      USA\n",
    "5        Ford        81         1      USA\n",
    "6     Ferrari        32         1    Italy\n",
    "7     Peugeot        53         0   France\n",
    "8     Renault        62         0   France\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and run the code to replace all values in the randVals column where `randVals = 0` with values of \\\\(2\\\\) in df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "df.randVals.replace(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and Run the code to replace all instances of  'Porsche' with 'Porsche Cayman', all instances of 'USA' with 'United States of America' and all instances of \\\\(0\\\\) with \\\\(10\\\\) in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "df.replace(to_replace = ['Porsche','USA',0]\n",
    "          ,value      = ['Porsche Cayman','United States of America',10])\n",
    "\n",
    "# The \"replace\" method can be used for both Series and DataFrame Objects.\n",
    "# instead of given two list (one to \"look for\" and the other \"to replace\"), we can give a dictionary\n",
    "\n",
    "df.replace(to_replace = { \"Company\": [\"Porsche\"]\n",
    "                         ,\"Country\": [\"USA\"]\n",
    "                         ,\"randVals\":[0]}\n",
    "\n",
    "            , value = { \"Company\": [\"Porsche Cayman\"]\n",
    "                       ,\"Country\": [\"United States of America\"]\n",
    "                       ,\"randVals\":[10]}\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panda has two tools to help dicretize and binning variables:\n",
    "\n",
    "#### pd.cut()\n",
    "```Python\n",
    "# Easily Create bins in variables using cut\n",
    "pd.cut(x, bins, right, labels, include_lowest)\n",
    "```\n",
    "\n",
    "| --- | --- | --- |\n",
    "| **Parameters** | **Type** | **Description** |\n",
    "| x | array-like | Needs to be 1-dimensional |\n",
    "| bins | list | Each element will define the bin edges |\n",
    "| right| Bool | Default = ``True``, indicates if the bins will include the rightmost edge |\n",
    "| labels | optional list | Specify the labels that will return in the bins |\n",
    "| include_lowest | Bool | Default = ``False``, indicates if the first bins will include the leftmost edge |\n",
    "\n",
    "#### pd.qcut()\n",
    "```Python\n",
    "# Easily create bins quantile based using qcut:\n",
    "pd.qcut(x, q, labels)\n",
    "```\n",
    "\n",
    "| --- | --- | --- |\n",
    "| **Parameters** | **Type** | **Description** |\n",
    "| x | array-like | Needs to be 1-dimensional |\n",
    "| q | int or list | if ``int`` specify number of quantiles to bin the data, also can specify ``list`` of quantiles |\n",
    "| labels | optional list | Specify the labels that will return in the bins |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following \\\\(2\\\\) lists:\n",
    "\n",
    "````\n",
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "age_bins = [18, 25, 35, 60, 100]\n",
    "````\n",
    "\n",
    "Write and run the code to create a dataframe of binned ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, \n",
    "        37, 31, 61, 45, 41, 32]\n",
    "        \n",
    "age_bins = [18, 25, 35, 60, 100]\n",
    "\n",
    "pd.cut(x = ages\n",
    "      ,bins = age_bins\n",
    "      # , labels = [1,2,3,4] # this line if you want to put labels for the bins, since the generated bins are pretty long\n",
    "      )\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that when we use median the binned distribution is better distributed (more values are in each interval) as opposed to when we use 'mean' (intervals are highly unbalanced) due to the disbalance in the data (there are lots of values below 100 and one single value of 1000). Similar to 'cut' we can use 'qcut', which defines the intervals based in quantiles instead of fixed values. Consider the random distribution below: \n",
    "\n",
    "````\n",
    "data = np.random.randn(10)\n",
    "````\n",
    "\n",
    "We can create \\\\(5\\\\) equally populated categories using qcut:\n",
    "````\n",
    "pd.qcut(data, 5)\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "data = np.random.randn(10)\n",
    "print(data)\n",
    "pd.qcut(data, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, the creation of \"dummies\" is commonly used. Consider the following dataframe:\n",
    "````\n",
    "   data1 key\n",
    "0      0   b\n",
    "1      1   b\n",
    "2      2   a\n",
    "3      3   c\n",
    "4      4   a\n",
    "5      5   b\n",
    "````\n",
    "`pandas` provides a very useful tool: `get_dummies`:\n",
    "\n",
    "```Python\n",
    "pd.get_dummies(x, prefix, drop_first)\n",
    "```\n",
    "\n",
    "|---|---|---|\n",
    "| **Parameters** | **Type** | **Description** |\n",
    "| x | Series or DataFrame | The data to get dummies |\n",
    "| prefix | optional str | Add a prefix to all created dummies |\n",
    "| drop_first | Bool | By default is ``False``. If ``True`` will drop the first level created |\n",
    "\n",
    "\n",
    "Write a script to generate a dataframe as the one above and create dummies based on the column <i>key</i> to transform the data. Describe what happens to the column 'key'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "## ANSWER:\n",
    "# The 'key' column was one-hot encoded. Every single\n",
    "# value in the key column became a new column and if the\n",
    "# corresponding row's value of 'key' matches that column\n",
    "# value, then a value of 1 is attributed to the result,\n",
    "# otherwise it is set to 0.\n",
    "\n",
    "df = pd.DataFrame({'key':['b','b','a','c','a','b'],\n",
    "                   'data1':range(6)})\n",
    "df\n",
    "#pd.get_dummies(df['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df['key'], prefix='KEY_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe a situation where you would want to use dummies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ANSWER`\n",
    "When extracting features from a dataset to use in building ML models, it is often useful to transform categorical features into numerical features which are required for most ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important technique in data wrangling involves string manipulation. Consider the following example: \n",
    "````\n",
    "a = 'My name is John Doe'\n",
    "a.split(' ')\n",
    "\n",
    ">>> ['My', 'name', 'is', 'John', 'Doe']\n",
    "````\n",
    "Now, consider the following: \n",
    "````\n",
    "dates = ['01-01-2019','01-02-2019','01-03-2019','01-04-2019']\n",
    "````\n",
    "Write a script to separate the first element of the following list of strings by '-' (returned as a list of three elements representing the day, month, and year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dates = ['01-01-2019','01-02-2019','01-03-2019','01-04-2019']\n",
    "dates[0].split('-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functional programming, generalize the previous expression to all elements in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "list(map((lambda x: x.split('-')),dates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script to create three separate lists: one list of days, one list of months and one list of years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "days   = list(map((lambda x: x.split('-')[0]),dates))\n",
    "months = list(map((lambda x: x.split('-')[1]),dates))\n",
    "years  = list(map((lambda x: x.split('-')[2]),dates))\n",
    "\n",
    "print(days, months, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `join` is the opposite operation of `split`. What's the output of this code? \n",
    "\n",
    "````\n",
    "a = 'My name is John Doe'\n",
    "' '.join(a.split(' ')) == a\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = 'My name is John Doe'\n",
    "\n",
    "' '.join(a.split(' ')) == a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful methods include `find`, `replace` and `count`. Predict the output of the following code:\n",
    "````\n",
    "a = 'Hi, my name is John Doe'\n",
    "a.find(',')\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "a = 'Hi, my name is John Doe'\n",
    "a.find(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"Date1\":[20190101,20190102],\"Date2\":['02/01/2019','03/01/2019'],\"Date3\":['12-20-2019', '12-21-2019']})\n",
    "#pd.to_datetime(df.Date1)\n",
    "df\n",
    "df.Date1 = pd.to_datetime(df.Date1, format = '%Y%m%d') #Try change %m to %M\n",
    "df.Date2 = pd.to_datetime(df.Date2, format = '%d/%m/%Y')  # put origin = '01/01/1900'\n",
    "df.Date3 = pd.to_datetime(df.Date3, format = '%m-%d-%Y')\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#df.Date3.dt.dayofweek\n",
    "#df.Date3.dt.dayofyear\n",
    "df.Date3.dt.day_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Difference in days between two dates\n",
    "df['nb_Days'] = df.Date3 - df.Date1\n",
    "\n",
    "# Difference in moths between two dates\n",
    "df['nb_months'] = ((df.Date3 - df.Date1)/np.timedelta64(1, 'M')).astype(\"int64\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import MonthEnd, MonthBegin, BDay\n",
    "\n",
    "# last day of the Month\n",
    "df['Date3_end'] = pd.to_datetime(df['Date3'], format=\"%Y%m\") + MonthEnd(1)\n",
    "\n",
    "# First day of the Month\n",
    "df['Date3_begin'] = pd.to_datetime(df['Date3'], format=\"%Y%m\") + MonthBegin(-1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=df.Date1.min(), end=df.Date3.max())\n",
    "holidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "weekmask = 'Mon Wed Fri Sat'\n",
    "\n",
    "bday_cust = CustomBusinessDay(holidays=holidays, weekmask=weekmask)\n",
    "\n",
    "pd.date_range(df.Date1.min(), df.Date3.max(), freq=bday_cust).size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
