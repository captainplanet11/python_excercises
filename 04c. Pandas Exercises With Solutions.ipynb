{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pandas is all about working with Data, we will be illustrating a lot of the Pandas concepts using a dataset. An entire modeling methodology is explored, starting from the basics of data exploration and treatment and ending by exploring different techniques for predictive analytics (logistic regression, decision trees, gradient boosting, etc.) The dataset we will use is a credit risk dataset containing credit card default information of clients in Taiwan. \n",
    "\n",
    "What follows is a brief description of the 25 variables in the dataset:\n",
    "<b>ID</b>: ID of each client\n",
    "<b>LIMIT_BAL</b>: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "<b>SEX</b>: Gender (1 = male; 2 = female).\n",
    "<b>EDUCATION</b>: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "<b>MARRIAGE</b>: Marital status (1 = married; 2 = single; 3 = others).\n",
    "<b>AGE</b>: Age (year).\n",
    "\n",
    "History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows:\n",
    "\n",
    "<b>PAY_0</b>:  the repayment status in September, 2005;\n",
    "<b>PAY_2</b>: the repayment status in August, 2005; . . .;\n",
    "<b>PAY_3</b>: . . .\n",
    "<b>PAY_4</b>: . . .\n",
    "<b>PAY_5</b>: . . .>\n",
    "<b>PAY_6</b>: the repayment status in April, 2005. \n",
    "The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
    "\n",
    "Amount of bill statement (NT dollar).\n",
    "\n",
    "<b>BILL_AMT1</b>: amount of bill statement in September, 2005;\n",
    "<b>BILL_AMT2</b>: amount of bill statement in August, 2005; . . .;\n",
    "<b>BILL_AMT3</b>: . . .;\n",
    "<b>BILL_AMT4</b>: . . .;\n",
    "<b>BILL_AMT5</b>: . . .;\n",
    "<b>BILL_AMT6</b>: amount of bill statement in April, 2005.\n",
    "\n",
    "Amount of previous payment (NT dollar).\n",
    "\n",
    "<b>PAY_AMT1</b>: amount paid in September, 2005;\n",
    "<b>PAY_AMT2</b>: amount paid in August, 2005; . . .;\n",
    "<b>PAY_AMT3</b>: . . .;\n",
    "<b>PAY_AMT4</b>: . . .;\n",
    "<b>PAY_AMT5</b>: . . .;\n",
    "<b>PAY_AMT6</b>: amount paid in April, 2005;\n",
    "<b>default.payment.next.month</b>: payment default (1 = yes; 2 = no)\n",
    "\n",
    "Source: UCI ML Repository: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, boto3, subprocess, re, sys, gc\n",
    "from botocore.client import Config\n",
    "\n",
    "print(\"All libraries successfully loaded!\")\n",
    "\n",
    "kms_key = os.environ['AW_S3_ENCRYPTION_KEY']\n",
    "\n",
    "bucket_name = os.environ['AW_S3_STORAGE_BUCKET']\n",
    "storage_key = os.environ['AW_S3_STORAGE_KEY'] + '/awdata/rawfiles/'\n",
    "full_s3_location = 's3://' + bucket_name + '/' + storage_key \n",
    "print(\"full_s3_location: '{}'\".format(full_s3_location))\n",
    "df_twn= pd.read_csv(full_s3_location + \"UCI_Credit_Card.csv\",nrows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore what we have learned so far about pandas dataframes, complete the following questions. \n",
    "\n",
    "1. Rename the column `default.payment.next.month` to simply `default`.\n",
    "\n",
    "2. Write a function to detect all the binary variables in the dataframe. \n",
    "\n",
    "3. Write a function to detect all the data types in a pandas dataframe and output the results as a dictionary (e.g. `{'float':[var1,...],...}`. \n",
    "\n",
    "4. Find the `median` and `max` values of the columns `'BILL_AMT1'`,`'BILL_AMT2'`,`'BILL_AMT3'`,`'BILL_AMT4'`,`'BILL_AMT5'` and `'BILL_AMT6'` for <b>each</b> value of `AGE`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# 1)Rename the column default.payment.next.month to simply default.\n",
    "df_twn.rename({'default.payment.next.month':'default'},axis=1,inplace=True)\n",
    "print(\"1.1: \",df_twn.columns)\n",
    "\n",
    "# 2)Write a function to detect all the binary variables in the dataframe.\n",
    "def CheckBinaries(df): \n",
    "    '''\n",
    "    Check for the binaries in a dataframe\n",
    "    '''\n",
    "    variables = df.columns.values\n",
    "    mask      = list(df.apply(lambda x: len(np.unique(x)) == 2))\n",
    "    return list(np.array(variables)[mask])\n",
    "\n",
    "# CheckBinaries(df_twn)\n",
    "print(\"\\n1.2: \",df_twn[CheckBinaries(df_twn)].head(5))\n",
    "\n",
    "# 3)Write a function to detect all the data types in a pandas dataframe and output the results as a dictionary (e.g. {'float':[var1,...],...}.\n",
    "dataTypes = list(set([df_twn.dtypes[i].name for i in df_twn.columns.values]))\n",
    "list_of_types = {str(j): [i for i in df_twn.columns.values if df_twn[i].dtypes == j] for j in dataTypes}\n",
    "print(\"\\n1.3: \",list_of_types)\n",
    "\n",
    "# 4)Find the median and max values of the columns 'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5' and 'BILL_AMT6' for each value of AGE.\n",
    "print(\"\\n1.4:\")\n",
    "z.show(df_twn.groupby(['AGE'])['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6'].agg(['mean','median']).reset_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of variation (CV) of a sample is defined as \\\\(\\sigma/\\mu\\\\) (the standard deviation divided by the mean). It is non-dimensional and is used to compare the spread of datasets. Another useful measure is the z-score, defined as \\\\((x - \\mu)/\\sigma\\\\). It measures the distance of a data point from the mean in terms of the standard deviation. This is also called standardization of data.\n",
    "\n",
    "1. Compute the coefficient of variation of the variable 'LIMIT_BAL' for each value of 'AGE'. \n",
    "\n",
    "2. Compute the z-scores of the variable 'PAY_AMT1'.\n",
    "\n",
    "3. Write a function to compute general statistical information on any given pandas dataframe column, and apply it to the BILL_AMT2 column. The output should follow this format (you can add other statistical functions as well!): \n",
    "````\n",
    "ComputeStats(dataframe,'BILL_AMT2')\n",
    ">>> {'max': 983931.0, 'min': -69777.0, 'median': 21197.0, 'mean': 49338.90635242128}\n",
    "````\n",
    "4. Write a function to calculate the number of unique values in each variable of a dataframe and apply it to the df_twn dataframe. \n",
    "\n",
    "5. Write a function to define a variable as continuous or categorical based on the number of unique values (you can use a threshold, e.g. more than 20 is considered continuous) and apply it to all columns in twn_df. The output should be a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "# 2.1) Compute the coefficient of variation of the variable ‘LIMIT_BAL’ for each value of ‘AGE’.\n",
    "print(\"2.1: \\n\",df_twn.groupby(['AGE'])['LIMIT_BAL'].apply(lambda x: x.std()/x.mean()).reset_index().rename(columns={'LIMIT_BAL':'CV'}))\n",
    "\n",
    "# 2.2) Compute the z-scores of the variable ‘PAY_AMT1’.\n",
    "z_scores = (df_twn.PAY_AMT1 - df_twn.PAY_AMT1.mean())/df_twn.PAY_AMT1.std()\n",
    "print(\"2.2: \\n\",pd.DataFrame({'PAY_AMT1': df_twn.PAY_AMT1,'z_scores':z_scores}).head(5))\n",
    "\n",
    "# 2.3) Define a function to compute general statistical information on any given pandas dataframe column. \n",
    "def ComputeStats(df,variable=None): \n",
    "    '''\n",
    "    Returns statistical information of any given variable\n",
    "    '''\n",
    "    stats_ = df.select_dtypes(['int64','float64']).apply(lambda x: {'mean': x.mean(),'median': x.median(),'max': x.max(),'min': x.min()},axis=0)\n",
    "    \n",
    "    if not variable: \n",
    "        return stats_\n",
    "    else:\n",
    "        return stats_[str(variable)]\n",
    "        \n",
    "print(\"\\n2.3: \",ComputeStats(df_twn,'BILL_AMT2'))\n",
    "\n",
    "\n",
    "# 2.4) Write a function to calculate the number of unique values in each variable.\n",
    "def CountUnique(df): \n",
    "    '''\n",
    "    Returns a series contining the number of unique values in each variable\n",
    "    '''\n",
    "    return df.apply(lambda x: len(np.unique(x))).reset_index().rename(columns={'index':'Column',0:'Count Unique'}) #Notice that the default column name of 0 is an integer and not a string.\n",
    "\n",
    "print(\"2.4: \\n\",CountUnique(df_twn))\n",
    "\n",
    "# 2.5) Write a function to define a variable as continuous or categorical based on the number of unique values\n",
    "def find_type_vars(df, lim):\n",
    "\n",
    "    unique = df.apply(lambda x: len(np.unique(x)))\n",
    "    mask = np.array(unique.values > 20) #Continuous\n",
    "    inverse_mask = np.where(mask == True, False, True) #Categorical\n",
    "    variables = df.columns.values\n",
    "    \n",
    "    return{\n",
    "            \"Continuous\": variables[mask].tolist(),\n",
    "            \"Categorical\": variables[inverse_mask].tolist()\n",
    "            }\n",
    "\n",
    "print(\"2.5: \\n\",find_type_vars(df_twn, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following random value generator: \n",
    "\n",
    "````\n",
    "def GenerateRandom(domain,values):\n",
    "    '''\n",
    "    Generates a list (of size 'values') of random integers between 0 and 'domain'-1\n",
    "    '''\n",
    "    import random\n",
    "    return np.unique([random.randint(0,domain-1) for x in range(values)])\n",
    "````\n",
    "\n",
    "1. Write a function that uses the function above, to **randomly** replace *N* values in a specified pandas dataframe column with missing (`np.nan`) values. Use your function to replace **50** values in the 'LIMIT_BAL' column of twn_df with missing values.\n",
    "\n",
    "2. Generalize the approach of 3.1 to apply the same function to a *multiple* columns.  Use your functions to replace **50** values in each of the following columns: 'PAY_AMT1','BILL_AMT3'\n",
    "\n",
    "3. Write a function to check the <b>number</b> of missing values in each column of a pandas dataframe. \n",
    "\n",
    "*hint* - Use ``df[col].isna().sum()`` to count how many NULL values are inside a Series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import random\n",
    "\n",
    "def GenerateRandom(domain,values): \n",
    "    return np.unique([random.randint(0,domain-1) for x in range(values)])\n",
    "\n",
    "def InsertMissing(data,variable,index):\n",
    "    random_df=data[variable]\n",
    "    random_df.loc[index,variable] = np.nan\n",
    "    data.loc[index,variable] = np.nan\n",
    "    return data\n",
    "    \n",
    "index_list = GenerateRandom(df_twn.shape[0],50) \n",
    "# print(index_list) #list of 50 random numbers matching size of one column in df_twn\n",
    "InsertMissing(df_twn,['LIMIT_BAL'],index_list)\n",
    "print (\"3.1: \\n\",df_twn['LIMIT_BAL'].head(5))\n",
    "\n",
    "InsertMissing(df_twn,['PAY_AMT1','BILL_AMT3'],index_list)\n",
    "print (\"3.2: \\n\",df_twn[['PAY_AMT1','BILL_AMT3']].head(5))\n",
    "\n",
    "def CountNaN(df):\n",
    "    '''\n",
    "    Counts the number of missing values per variable\n",
    "    '''\n",
    "    return pd.DataFrame([(i,df[i].isna().sum()) for i in df.columns.values],columns=['Column', '# Missing'])\n",
    "\n",
    "print (\"3.3: \\n\",CountNaN(df_twn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to detect the records in our dataframe that are outliers. One method is to use the inter quartile range (IQR). The IQR is given by \\\\(IQR = Q_{3} - Q_{1}\\\\), where \\\\(Q_{3}\\\\) is the third quartile and \\\\(Q_{1}\\\\) is the first quartile. Outliers are records which lie outside of the following range: \\\\( [Q_{1} - 1.5 \\cdot \\mathrm{IQR}, Q_{3} + 1.5 \\cdot \\mathrm{IQR}] \\\\), that is \\\\( a < Q_{1} - 1.5 \\cdot \\mathrm{IQR} \\\\) or \\\\( b > Q_{3} + 1.5 \\cdot \\mathrm{IQR} \\\\).  This is same logic that is used to draw box plots:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/11250/1*2c21SkzJMf3frPXPAR_gZA.png\" title=\"Source: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51\" width=\"500\"/>\n",
    "\n",
    "1. Define a function to compute the IQR outliers of a given pandas dataframe column (return the index of each outlier)\n",
    "\n",
    "2. Write a function to <b>replace</b> the outliers of all <i>continuous</i> variables with `np.nan` (remember we have defined a function to determine whether a variable is continuous or not in Question 2.5 of this notebook).\n",
    "\n",
    "3. Write a function to replace results of question 4.2 with the median value of each column.  Double check that no missing values remain by using the function you wrote in question 3.3.\n",
    "\n",
    "\n",
    "*hint* - use ``df.quantile(q)`` to calculate the \\\\(q\\\\) quantile, where \\\\(0 <= q <= 1\\\\)\n",
    "\n",
    "\n",
    "<b>Bonus</b> Rewrite the function from question 4.1 using z-score and mod-z-score methods for outlier detection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    " \n",
    "# 4.1) Define a function to compute the IQR outliers of a given pandas dataframe column (return the index of each outlier)\n",
    "\n",
    "def OutlierIQR(data):\n",
    "    '''\n",
    "    Computes the indices of the corresponding outlier values according to IQR\n",
    "    '''\n",
    "    Q1 = data.quantile(q = 0.25)\n",
    "    Q3 = data.quantile(q = 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    LB = Q1 - (IQR*1.5)\n",
    "    UB = Q3 + (IQR*1.5)\n",
    "    outlier = np.where((data > UB) | (data < LB))\n",
    "    return outlier[0]\n",
    " \n",
    "\n",
    "print(\"4.1: \",OutlierIQR(df_twn['BILL_AMT1']))\n",
    "\n",
    "# 4.2: Write a function to replace the outliers of all continuous variables with np.nan \n",
    "def find_type_vars(df, lim):\n",
    "\n",
    "    unique = df.apply(lambda x: len(np.unique(x)))\n",
    "    mask = np.array(unique.values > 20) #Continuous\n",
    "    inverse_mask = np.where(mask == True, False, True) #Categorical\n",
    "    variables = df.columns.values\n",
    "    \n",
    "    return{\n",
    "            \"Continuous\": variables[mask].tolist(),\n",
    "            \"Categorical\": variables[inverse_mask].tolist()\n",
    "            }\n",
    "\n",
    "def RemoveOutliers(data,variables):\n",
    "    '''\n",
    "    Function to remove outliers and input NaN on those values\n",
    "    '''\n",
    "    for var in variables: \n",
    "        # print(var)\n",
    "        index_list = OutlierIQR(data[str(var)])\n",
    "        data.loc[index_list,str(var)] = np.nan\n",
    "\n",
    "print(\"\\n 4.2:\")\n",
    "continuous=find_type_vars(df_twn,20)['Continuous']\n",
    "RemoveOutliers(df_twn,continuous)\n",
    "print(df_twn[continuous].head(5))\n",
    "\n",
    "# 4.3: Write a function to replace results of question 4.2 with the median value of each column.  Double check that no missing values remain by using the function you wrote in question 3.3.\n",
    "def InputMethod(x): \n",
    "    return {'median':x.median(),'mean':x.mean(),'zero':0}\n",
    "    \n",
    "def ReplaceMissing(data): \n",
    "    '''\n",
    "    Function to replace missing values using a dictionary\n",
    "    '''\n",
    "    return data.fillna(data.median())\n",
    "    \n",
    "\n",
    "df_twn2 = ReplaceMissing(df_twn)\n",
    "print(df_twn2[continuous].head(5))\n",
    "def CountNaN(df):\n",
    "    '''\n",
    "    Counts the number of missing values per variable\n",
    "    '''\n",
    "    return pd.DataFrame([(i,df[i].isna().sum()) for i in df.columns.values],columns=['Column', '# Missing'])\n",
    "\n",
    "print (\"4.3: Count Missing in df_twn\\n\",CountNaN(df_twn))\n",
    "print (\"4.3: Count Missing in df_twn2\\n\",CountNaN(df_twn2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
